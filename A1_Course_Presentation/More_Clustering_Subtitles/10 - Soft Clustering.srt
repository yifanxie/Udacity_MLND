1
00:00:00,510 --> 00:00:03,220
So to do soft clustering, we're going to use a similar trick to what

2
00:00:03,220 --> 00:00:08,430
we've used in some of the other lectures, which is to lean on probability theory

3
00:00:08,430 --> 00:00:13,580
so that now points instead of coming from one cluster or another cluster can

4
00:00:13,580 --> 00:00:16,560
be probabilistically from one of multiple possible

5
00:00:16,560 --> 00:00:18,530
clusters. Does that seem like a good idea?

6
00:00:18,530 --> 00:00:21,270
>> It does. I feel a song coming on. Lean on

7
00:00:21,270 --> 00:00:24,730
probability when you're not strong. [INAUDIBLE]

8
00:00:24,730 --> 00:00:26,940
one thing. No, that doesn't work.

9
00:00:26,940 --> 00:00:28,240
>> Yeah, that almost worked.

10
00:00:28,240 --> 00:00:28,780
>> Almost.

11
00:00:28,780 --> 00:00:30,580
>> Yeah. So that's because it's soft

12
00:00:30,580 --> 00:00:32,630
clustering or soft assignments instead of hard ones.

13
00:00:32,630 --> 00:00:33,580
>> I like it.

14
00:00:33,580 --> 00:00:35,700
>> All right. So to do this, we're

15
00:00:35,700 --> 00:00:38,900
going to have to connect up a probabilistic generator

16
00:00:38,900 --> 00:00:40,580
of process with the data that we actually

17
00:00:40,580 --> 00:00:44,000
observed. So let's assume, and there's many ways

18
00:00:44,000 --> 00:00:47,130
to go down this road, but we're going to go down the road this way. Assume that

19
00:00:47,130 --> 00:00:49,300
the data was generated by, what happens is

20
00:00:49,300 --> 00:00:53,270
we're going to select one of K possible Gaussian distributions.

21
00:00:53,270 --> 00:00:54,440
So we're going to imagine that the data's

22
00:00:54,440 --> 00:00:57,950
going to be generated by draws from Gaussians, from normals.

23
00:00:57,950 --> 00:00:58,755
>> Mm-hm.

24
00:00:58,755 --> 00:01:01,620
>> Let's assume that we know the variants, sigma

25
00:01:01,620 --> 00:01:06,240
square, and that the K Gaussians are sampled from uniformly.

26
00:01:06,240 --> 00:01:09,160
>> Okay. And then what we're going to do is that given

27
00:01:09,160 --> 00:01:11,840
that Gaussian we're going to select an actual point, an actual data

28
00:01:11,840 --> 00:01:15,430
point in the space from that Gaussian. And then we repeat

29
00:01:15,430 --> 00:01:18,310
that n times. So if n is bigger than K then

30
00:01:18,310 --> 00:01:20,900
we're going to see some points that come from the same Gaussian,

31
00:01:20,900 --> 00:01:23,450
and if those Gaussians are well separated they're going to look like clusters.

32
00:01:23,450 --> 00:01:25,090
>> Assuming they have very different means.

33
00:01:25,090 --> 00:01:28,770
>> Alright. That's what I mean by, we'll separate it. Yeah exactly so.

34
00:01:28,770 --> 00:01:30,090
>> Oh, yeah, yeah, yeah, okay. Good.

35
00:01:30,090 --> 00:01:33,310
>> Alright, and in particular, what we'd like to do now is say, alright,

36
00:01:33,310 --> 00:01:35,290
well, now what we're really, happening, is,

37
00:01:35,290 --> 00:01:36,960
we're given the data, we're thinking kind

38
00:01:36,960 --> 00:01:39,600
Bayesianly, right, we're given the data and we want to try to figure out

39
00:01:39,600 --> 00:01:41,580
what the clusters would have been to

40
00:01:41,580 --> 00:01:43,380
have generated that data. So we're going to try

41
00:01:43,380 --> 00:01:45,960
to find a hypothesis, which is this case is just going to

42
00:01:45,960 --> 00:01:51,973
be a collection of k means, not to be confused with K-means.

43
00:01:51,973 --> 00:01:52,030
>> Mm.

44
00:01:52,030 --> 00:01:56,055
>> That maximizes the probability of the data. Right? So find me

45
00:01:56,055 --> 00:02:00,240
K-mu values, which are the means of those Gaussian distributions. So that

46
00:02:00,240 --> 00:02:04,600
the probability of the data given that hypothesis is as high as

47
00:02:04,600 --> 00:02:09,009
possible. And this is an ML hypothesis. And ML of course stands for

48
00:02:09,009 --> 00:02:09,449
Michael Lipman.

49
00:02:09,449 --> 00:02:11,640
>> I don't think that's right.

50
00:02:11,640 --> 00:02:12,140
>> Machine learning.

51
00:02:13,460 --> 00:02:15,510
>> That's closer, but not quite right.

52
00:02:15,510 --> 00:02:16,270
>> Maximum likelihood.

53
00:02:16,270 --> 00:02:17,320
>> That I think is correct.

54
00:02:17,320 --> 00:02:20,760
>> Alright. So that's now the problem setup. I didn't actually

55
00:02:20,760 --> 00:02:23,620
give you an algorithm for doing this, but presumably it's going

56
00:02:23,620 --> 00:02:26,800
to depend on various kinds of things and probability theory and

57
00:02:26,800 --> 00:02:29,430
optimization, but is it sort of clear what we're shooting for now?

58
00:02:29,430 --> 00:02:31,250
>> It is, it is. And I actually think the

59
00:02:31,250 --> 00:02:34,090
fact that we're looking for k means probably means that

60
00:02:34,090 --> 00:02:37,380
we are going to end up tying it back to k means. Maybe so, but

61
00:02:37,380 --> 00:02:40,830
again, it's a softer kind of k means, it's a softer, gentler kind of k means.

62
00:02:40,830 --> 00:02:41,420
>> Mm.

63
00:02:41,420 --> 00:02:44,840
>> I think some people call it K Gaussians. Does that sound right?

64
00:02:44,840 --> 00:02:45,730
>> No.

65
00:02:45,730 --> 00:02:47,560
>> Alright then.

66
00:02:47,560 --> 00:02:49,220
>> I mean it sounds correct, but it doesn't sound right.

67
00:02:49,220 --> 00:02:53,339
>> [LAUGH] Alright then.

