1
00:00:00,710 --> 00:00:01,710
So, this is going to lead us to

2
00:00:01,710 --> 00:00:04,575
the concept of expectation maximization. So, expectation

3
00:00:04,575 --> 00:00:06,553
maximization is actually, at an algorithmic level,

4
00:00:06,553 --> 00:00:08,610
it's surprisingly similar to K means. So, what

5
00:00:08,610 --> 00:00:10,365
we are going to do is, we're going to

6
00:00:10,365 --> 00:00:12,520
tick-tock back and forth between 2 different

7
00:00:12,520 --> 00:00:14,830
probabilistic calculations. So, you see that? I

8
00:00:14,830 --> 00:00:15,890
kind of drew it like the other one.

9
00:00:15,890 --> 00:00:20,351
>> Mm Hm. The names of the 2 phases are expectation,

10
00:00:20,351 --> 00:00:24,680
and maximization. Sort of you know, our name is our algorithim.

11
00:00:24,680 --> 00:00:25,430
>> I like that.

12
00:00:26,490 --> 00:00:28,240
>> So, what they're going to do is, we're going to

13
00:00:28,240 --> 00:00:32,400
move back and forth between a soft clustering, and computing

14
00:00:32,400 --> 00:00:34,160
the means from the soft cluster. So the soft

15
00:00:34,160 --> 00:00:37,270
clustering goes like this. This probablisitic indicator variable, Z I

16
00:00:37,270 --> 00:00:43,290
J. Represents the likelihood that data element I comes

17
00:00:43,290 --> 00:00:45,850
from cluster J. And so, the way we're going to do

18
00:00:45,850 --> 00:00:48,440
that, since we're in the maximum likelihood setting, is to

19
00:00:48,440 --> 00:00:51,550
use Bayes' rule, and say, well, that's going to be proportional

20
00:00:51,550 --> 00:00:54,440
to the probability that data element I was

21
00:00:54,440 --> 00:00:56,460
produced by cluster J. And then we have a

22
00:00:56,460 --> 00:00:58,780
normalization factor. Normally, we'd also have the prior

23
00:00:58,780 --> 00:01:00,600
in there. So why is the prior gone Charles?

24
00:01:00,600 --> 00:01:03,880
>> Well, because you said it was the maximum likelihood. Scenario.

25
00:01:03,880 --> 00:01:05,420
>> Yeah, right. We talked about how that

26
00:01:05,420 --> 00:01:06,910
just meant that it was uniform and that

27
00:01:06,910 --> 00:01:08,730
allowed us to just leave that component out.

28
00:01:08,730 --> 00:01:10,520
It's not going to have any impact on the normalization.

29
00:01:10,520 --> 00:01:11,250
>> Right.

30
00:01:11,250 --> 00:01:13,030
>> So that's what the Z step is. Is if we had

31
00:01:13,030 --> 00:01:16,600
the clusters, if we knew where the means were, then we could compute

32
00:01:16,600 --> 00:01:18,210
how likely it is that the data would come from

33
00:01:18,210 --> 00:01:20,680
the means, and that's just this calculation here. So that's computing

34
00:01:20,680 --> 00:01:24,130
the expectation. Defining the Z varaibles from the muse. The centers.

35
00:01:24,130 --> 00:01:28,044
We're going to pass that information. That clustering information, Z, over to

36
00:01:28,044 --> 00:01:31,560
the maximization step. What the maximization is going to say is,

37
00:01:31,560 --> 00:01:33,780
okay, well if that's the clustering, we can compute the means

38
00:01:33,780 --> 00:01:36,550
from those clusters. All we have to do is just take

39
00:01:36,550 --> 00:01:41,720
the average variable value. Right? So the average of the Xi's.

40
00:01:41,720 --> 00:01:45,008
Within each cluster J. What's the likelihood it came from cluster J and

41
00:01:45,008 --> 00:01:48,370
then again, we have to normalize. If you think of this as being

42
00:01:48,370 --> 00:01:51,810
a 0 1 indicator variable, then really it is just the average of

43
00:01:51,810 --> 00:01:54,880
the things we assign to that cluster. But here, we actually are kind

44
00:01:54,880 --> 00:01:57,560
of soft assigning, so we could have half of one of the data

45
00:01:57,560 --> 00:02:00,060
points in there, and it only counts half towards the average, and we

46
00:02:00,060 --> 00:02:03,950
could have a tenth in another place, and a whole value in another

47
00:02:03,950 --> 00:02:06,610
place, and so we're just doing this weighted average of the data points.

48
00:02:06,610 --> 00:02:07,620
>> So,

49
00:02:07,620 --> 00:02:08,850
can I ask you a question, Michael?

50
00:02:08,850 --> 00:02:09,620
>> Yeah, shoot.

51
00:02:09,620 --> 00:02:10,860
>> So, this makes sense to me, and I, and I

52
00:02:10,860 --> 00:02:14,630
even get that for the Gaussian case, the z i variable will

53
00:02:14,630 --> 00:02:17,340
always be non 0 in the end, because there's always some

54
00:02:17,340 --> 00:02:21,350
probability. They come from some Gaussian because they have infinite extent. So

55
00:02:21,350 --> 00:02:23,510
I, this all makes sense to me. Is there a way

56
00:02:23,510 --> 00:02:26,740
to take exactly this algorithm and turn it into k means? I'm

57
00:02:26,740 --> 00:02:29,280
staring at it, and it feels like if all your probabilities

58
00:02:29,280 --> 00:02:33,120
were ones and zeroes, you would end up with exactly k means.

59
00:02:33,120 --> 00:02:33,440
I think.

60
00:02:33,440 --> 00:02:35,970
>> I dunno, I never really thought about that. Let's

61
00:02:35,970 --> 00:02:37,830
think about that for a moment. Certainly, the case, if

62
00:02:37,830 --> 00:02:40,750
all the z variables were 0, 1, then the maximization

63
00:02:40,750 --> 00:02:43,290
set would be the means, which is what k means does.

64
00:02:43,290 --> 00:02:48,080
>> Mm-hm. Then, what would happen? We send these means back, and what we do

65
00:02:48,080 --> 00:02:53,760
in k-means is we say, each data point belongs to it's closest center.

66
00:02:53,760 --> 00:02:54,541
>> Mm-hm.

67
00:02:54,541 --> 00:02:58,380
>> Which is very similar actually to what this does. Except that

68
00:02:58,380 --> 00:03:01,030
here we then make it proportional. So I guess it would

69
00:03:01,030 --> 00:03:04,668
exactly that if we made these clustering assignments, pushed them to

70
00:03:04,668 --> 00:03:08,090
0 or 1 depending on which was the most likely cluster.

71
00:03:08,090 --> 00:03:10,730
Right, so if you made it so that the probability of you

72
00:03:10,730 --> 00:03:14,470
being to a cluster actually depends upon all the clusters, and

73
00:03:14,470 --> 00:03:16,500
you always got a 1 over 0. Basically you did, this was

74
00:03:16,500 --> 00:03:18,550
like a hidden argmax kind of a thing, or a

75
00:03:18,550 --> 00:03:23,241
hidden max or something. Then you would end up with exactly k-means.

76
00:03:23,241 --> 00:03:23,570
>> I think

77
00:03:23,570 --> 00:03:23,860
you're right.

78
00:03:23,860 --> 00:03:24,250
>> Huh.

79
00:03:24,250 --> 00:03:25,150
>> Yeah, I never thought about that.

80
00:03:25,150 --> 00:03:25,880
>> Okay.

81
00:03:25,880 --> 00:03:27,924
>> So it really does end up being an awful lot like

82
00:03:27,924 --> 00:03:31,910
the k-means algorithm, which is improving

83
00:03:31,910 --> 00:03:33,690
in the error metric, this squared error

84
00:03:33,690 --> 00:03:37,270
metric. This is actually going to be improving in a probabilistic metric, right.

85
00:03:37,270 --> 00:03:40,210
The, the data is going to be more and more likely over time.

86
00:03:40,210 --> 00:03:41,230
>> That makes sense.

