1
00:00:00,200 --> 00:00:03,860
So we talked about the equations that defined expectation maximization,

2
00:00:03,860 --> 00:00:06,930
and we stepped through an example with some actual data

3
00:00:06,930 --> 00:00:08,760
in the sense that it was data points. They weren't

4
00:00:08,760 --> 00:00:11,632
actual, measured data points. But what I'd like to talk

5
00:00:11,632 --> 00:00:13,810
about now for a moment is some of the properties

6
00:00:13,810 --> 00:00:17,600
of the EM algorithm more generally. So one of the

7
00:00:17,600 --> 00:00:20,630
things that's good about it is that each time the

8
00:00:20,630 --> 00:00:26,085
iteration of EM runs, the likelihood of the data is monotonically

9
00:00:26,085 --> 00:00:28,620
non-decreasing, right? So it's not getting worse.

10
00:00:28,620 --> 00:00:30,830
Generally it's finding higher and higher likelihoods

11
00:00:30,830 --> 00:00:35,730
and it's kind of moving in a good direction that way. Unfortunately, even though

12
00:00:35,730 --> 00:00:39,650
that's true, it is not the case that the algorithm has to converge. I

13
00:00:39,650 --> 00:00:42,540
mean have you ever seen it not converge? I've never seen it not converge.

14
00:00:42,540 --> 00:00:44,770
>> No, because usually there's some kind of step

15
00:00:44,770 --> 00:00:46,280
that you take and you just, you make the

16
00:00:46,280 --> 00:00:48,570
weight lower and lower. So yeah, or something like

17
00:00:48,570 --> 00:00:50,810
that. You know, no, I've never seen it not converge.

18
00:00:50,810 --> 00:00:51,830
>> So it

19
00:00:51,830 --> 00:00:53,380
doesn't have to converge. I think you can

20
00:00:53,380 --> 00:00:55,850
construct really strange examples that make it do

21
00:00:55,850 --> 00:00:57,500
that. But on the other hand, so even

22
00:00:57,500 --> 00:01:00,320
though it doesn't converge, it can't diverge, right?

23
00:01:00,320 --> 00:01:02,030
It can't be that these numbers blow up

24
00:01:02,030 --> 00:01:04,400
and become infinitely large because it really is

25
00:01:04,400 --> 00:01:07,150
working in the space of probabilities. And it's,

26
00:01:07,150 --> 00:01:08,980
it's pretty well behaved as far as that's concerned.

27
00:01:08,980 --> 00:01:12,460
>> That's a difference between in k means, right? So, the argument for

28
00:01:12,460 --> 00:01:16,810
k means, if I recall, which feels like was about a week ago,

29
00:01:16,810 --> 00:01:19,980
when we talked about this. [LAUGH] But of course, it was, it was merely seconds

30
00:01:19,980 --> 00:01:22,264
ago. Is that there is a finite number

31
00:01:22,264 --> 00:01:24,748
of configurations and k means and since you

32
00:01:24,748 --> 00:01:28,364
are never getting worse in our error metric. So long as you have some way

33
00:01:28,364 --> 00:01:30,260
of breaking ties, eventually you have to stop.

34
00:01:30,260 --> 00:01:31,970
And, so, that's how you got convergence, right?

35
00:01:31,970 --> 00:01:32,670
>> Yeah.

36
00:01:32,670 --> 00:01:37,920
>> So, in EM, I guess the configurations are probabilities and I guess

37
00:01:37,920 --> 00:01:40,800
there is an infinite number of those. Yet you can do more than guess.

38
00:01:40,800 --> 00:01:41,820
>> So in fact,

39
00:01:41,820 --> 00:01:43,070
there are an infinite number of those.

40
00:01:43,070 --> 00:01:44,090
>> Exactly.

41
00:01:44,090 --> 00:01:47,410
>> It wouldn't necessarily follow that you wouldn't converge

42
00:01:47,410 --> 00:01:50,050
from that. But that alone is, is one big difference,

43
00:01:50,050 --> 00:01:52,390
I guess, between the k means and the, the

44
00:01:52,390 --> 00:01:53,910
EM. That's the trade off you get for being able

45
00:01:53,910 --> 00:01:57,460
to put probabilities on things. So you've got an

46
00:01:57,460 --> 00:02:01,640
infinite number of configurations. You never do worse but you're

47
00:02:01,640 --> 00:02:04,430
always trying to move closer and closer. So, I guess

48
00:02:04,430 --> 00:02:06,960
what could happen is you could keep moving closer every

49
00:02:06,960 --> 00:02:10,070
single time, but because there's a infinite number of configurations,

50
00:02:10,070 --> 00:02:13,500
the step by which you get better could keep getting smaller,

51
00:02:13,500 --> 00:02:17,150
and so you never actually approach the final best configuration. I

52
00:02:17,150 --> 00:02:20,120
suppose that's possible. But for all intents and purposes, it converges.

53
00:02:20,120 --> 00:02:23,310
>> Right. Exactly. However, it can get stuck. And this

54
00:02:23,310 --> 00:02:26,010
you see all the time. I almost never not see it

55
00:02:26,010 --> 00:02:29,590
do this. Which is to say that, if there's multiple

56
00:02:29,590 --> 00:02:32,040
ways of assigning things to clusters, it could find a way

57
00:02:32,040 --> 00:02:35,630
that doesn't have very good likelihood but can't really improve

58
00:02:35,630 --> 00:02:38,410
on very well. So there's a local optima problem that

59
00:02:38,410 --> 00:02:40,190
is pretty common, and so what do we do when

60
00:02:40,190 --> 00:02:43,850
we, get stuck in local optima with a randomized algorithm?

61
00:02:43,850 --> 00:02:44,300
>> Cry.

62
00:02:44,300 --> 00:02:45,480
>> No.

63
00:02:45,480 --> 00:02:48,210
>> We take all of our toys home and randomly restart.

64
00:02:48,210 --> 00:02:48,870
>> There we go.

65
00:02:48,870 --> 00:02:49,540
>> Okay.

66
00:02:49,540 --> 00:02:51,980
>> And the last thing to mention is this, is, is what you

67
00:02:51,980 --> 00:02:57,160
just suggested a moment ago in the previous slide. Which is that, it's nothing,

68
00:02:57,160 --> 00:02:59,720
there's nothing specific about Gaussians in here. It really is an

69
00:02:59,720 --> 00:03:02,810
algorithm that can be applied anytime that we can work with

70
00:03:02,810 --> 00:03:06,770
probability distribution. And so there's just a ton of different algorithms

71
00:03:06,770 --> 00:03:08,440
that work in different scenarios

72
00:03:08,440 --> 00:03:11,090
by defining different probability distributions, and

73
00:03:11,090 --> 00:03:13,010
then all you have to do is figure out what the

74
00:03:13,010 --> 00:03:15,930
E step and the M step are. How do you expectation

75
00:03:15,930 --> 00:03:19,070
to work out the probability of the latent variables. And then,

76
00:03:19,070 --> 00:03:22,470
how do you do maximization to use those latent variables to

77
00:03:22,470 --> 00:03:25,930
estimate parameters? And usually it's the case

78
00:03:25,930 --> 00:03:28,260
that it's the estimation that's expensive. It's

79
00:03:28,260 --> 00:03:31,530
difficult because it involves probabilistic inference. Right?

80
00:03:31,530 --> 00:03:33,320
So it's just like Bayes net stuff.

81
00:03:33,320 --> 00:03:33,690
>> Mm.

82
00:03:33,690 --> 00:03:35,450
>> And the maximization is often just,

83
00:03:35,450 --> 00:03:37,360
you know, counting things. But it isn't,

84
00:03:37,360 --> 00:03:40,805
in general, the case that it's always harder to do E than M. There's some

85
00:03:40,805 --> 00:03:45,220
well-known examples where M is hard and E is actually quite easy. You know, for

86
00:03:45,220 --> 00:03:47,640
any given problem you have some work to do to derive what the E and

87
00:03:47,640 --> 00:03:49,510
the M steps are. But it's very general, it's a

88
00:03:49,510 --> 00:03:51,310
really it's a good tool to have in your toolbox.

89
00:03:51,310 --> 00:03:53,230
>> I like that. So, basically it's not that

90
00:03:53,230 --> 00:03:55,440
hard because it's just a small matter of math programming.

91
00:03:55,440 --> 00:03:56,300
>> Indeed.

