{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_memo = '''Milt, we're gonna need to go ahead and move you downstairs into storage B. We have some new people coming in, and we need all the space we can get. So if you could just go ahead and pack up your stuff and move it down there, that would be terrific, OK?\n",
    "Oh, and remember: next Friday... is Hawaiian shirt day. So, you know, if you want to, go ahead and wear a Hawaiian shirt and jeans.\n",
    "Oh, oh, and I almost forgot. Ahh, I'm also gonna need you to go ahead and come in on Sunday, too...\n",
    "Hello Peter, whats happening? Ummm, I'm gonna need you to go ahead and come in tomorrow. So if you could be here around 9 that would be great, mmmk... oh oh! and I almost forgot ahh, I'm also gonna need you to go ahead and come in on Sunday too, kay. We ahh lost some people this week and ah, we sorta need to play catch up.\n",
    "'''\n",
    "\n",
    "\n",
    "corrupted_memo = '''Yeah, I'm gonna --- you to go ahead --- --- complain about this. Oh, and if you could --- --- and sit at the kids' table, that'd be --- \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for this time, for that time, for this job, for those item,\\nfor those time'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for this time, for that time, for this job, for those item,\\nfor those time'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_memo2 = '''for this time, for that time, for this job, for those item,\n",
    "for those time'''\n",
    "sample_memo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You guessed be after you, where we guessed ['go'].\n",
    "Here was our whole calculation: {'be': 2, 'play': 3, 'just': 2, 'into': 1, 'to,': 1, 'go': 12, 'if': 1} [[12, 'go'], [3, 'play'], [2, 'just'], [2, 'be'], [1, 'to,'], [1, 'into'], [1, 'if']]\n",
    "You guessed a after and, where we guessed ['in'].\n",
    "Here was our whole calculation: {'a': 1, 'Oh,': 1, 'we': 1, 'up': 1, 'almost': 4, 'it': 2, 'next': 1, 'sorta': 1, 'can': 1, 'in': 9, 'need': 1, 'you': 2} [[9, 'in'], [4, 'almost'], [2, 'you'], [2, 'it'], [1, 'we'], [1, 'up'], [1, 'sorta'], [1, 'next'], [1, 'need'], [1, 'can'], [1, 'a'], [1, 'Oh,']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#   Maximum Likelihood Hypothesis\n",
    "#\n",
    "#\n",
    "#   In this quiz we will find the maximum likelihood word based on the preceding word\n",
    "#\n",
    "#   Fill in the NextWordProbability procedure so that it takes in sample text and a word,\n",
    "#   and returns a dictionary with keys the set of words that come after, whose values are\n",
    "#   the number of times the key comes after that word.\n",
    "#   \n",
    "#   Just use .split() to split the sample_memo text into words separated by spaces.\n",
    "def NextWordProbability(sampletext,word):\n",
    "#     sampletext=re.sub('[\\W]+', ' ', sampletext)\n",
    "#     word_array=np.array(sampletext.lower().split())\n",
    "    word_array=np.array(sampletext.split())\n",
    "    word_pos=np.argwhere(word_array==word)\n",
    "    next_word=word_array[word_pos+1]\n",
    "    next_word_unique, next_word_count=np.unique(next_word, return_counts=True)\n",
    "    next_word_prob=np.float32(next_word_count)/next_word_count.sum()\n",
    "#     output_dict=dict(zip(next_word_unique,next_word_prob))\n",
    "    return next_word_unique, next_word_count\n",
    "#     return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# def LaterWords(sample,word,distance):\n",
    "#     '''@param sample: a sample of text to draw from\n",
    "#     @param word: a word occuring before a corrupted sequence\n",
    "#     @param distance: how many words later to estimate (i.e. 1 for the next word, 2 for the word after that)\n",
    "#     @returns: a single word which is the most likely possibility\n",
    "#     '''\n",
    "#     output_dict={}\n",
    "#     minus_one_word, minus_one_count=NextWordProbability(sample, word)\n",
    "#     minus_one_dict=dict(zip(minus_one_word,minus_one_count))\n",
    "#     # TODO: Given a word, collect the relative probabilities of possible following words\n",
    "#     # from @sample. You may want to import your code from the maximum likelihood exercise. \n",
    "#     if distance==1: return minus_one_dict\n",
    "\n",
    "#     # TODO: Repeat the above process--for each distance beyond 1, evaluate the words that\n",
    "#     # might come after each word, and combine them weighting by relative probability\n",
    "#     # into an estimate of what might appear next.\n",
    "#     current_dist=2\n",
    "#     current_dist_dict=minus_one_dict\n",
    "# #     print(current_dist_dict)\n",
    "#     while current_dist<=distance:\n",
    "#         print('calculating probablity for distance {: d}'.format(current_dist))\n",
    "#         key_counter=1\n",
    "#         for key, prob in current_dist_dict.items():\n",
    "\n",
    "#             key_minus_one_word, key_minus_one_count=NextWordProbability(sample, key)\n",
    "#             if key_counter==1:\n",
    "#                 current_dist_word = key_minus_one_word\n",
    "#                 current_dist_count = key_minus_one_count\n",
    "#             else:\n",
    "#                 print current_dist_word, key_minus_one_word\n",
    "#                 current_dist_word=np.concatenate((current_dist_word, key_minus_one_word))\n",
    "#                 current_dist_count=np.concatenate((current_dist_count, key_minus_one_count))\n",
    "#                 print current_dist_word\n",
    "#             key_counter+=1\n",
    "#         current_dist+=1\n",
    "# #     arg_wordmax=\n",
    "#     argmax=np.argmax(current_dist_count)\n",
    "#     output=current_dist_word[argmax]\n",
    "#     return current_dist_word, current_dist_count, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_sum(a,b):\n",
    "    unique_list=[]\n",
    "    unique_sum=[]\n",
    "    for i in np.unique(a):\n",
    "        unique_list.append(i)\n",
    "        unique_sum.append(b[np.argwhere(a==i)].sum())\n",
    "    return np.array(unique_list), np.array(unique_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.array(['ah', 'come', 'i', 'jeans', 'come', 'jean'])\n",
    "b=np.array([1,3,2,1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ah', 'come', 'i', 'jean', 'jeans'], \n",
       "       dtype='|S5'), array([1, 6, 2, 2, 1]))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_sum(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def LaterWords(sample,word,distance):\n",
    "    '''@param sample: a sample of text to draw from\n",
    "    @param word: a word occuring before a corrupted sequence\n",
    "    @param distance: how many words later to estimate (i.e. 1 for the next word, 2 for the word after that)\n",
    "    @returns: a single word which is the most likely possibility\n",
    "    '''\n",
    "#     print(word, distance)\n",
    "    output_dict={}\n",
    "    minus_one_word, minus_one_prob=NextWordProbability(sample, word)\n",
    "    minus_one_dict=dict(zip(minus_one_word,minus_one_prob))\n",
    "    # TODO: Given a word, collect the relative probabilities of possible following words\n",
    "    # from @sample. You may want to import your code from the maximum likelihood exercise. \n",
    "    if distance==1: return minus_one_dict\n",
    "\n",
    "    # TODO: Repeat the above process--for each distance beyond 1, evaluate the words that\n",
    "    # might come after each word, and combine them weighting by relative probability\n",
    "    # into an estimate of what might appear next.\n",
    "    current_dist=2\n",
    "    current_dist_dict=minus_one_dict\n",
    "#     print(current_dist_dict)\n",
    "    while current_dist<=distance:\n",
    "        # print('calculating probablity for distance {: d}'.format(current_dist))\n",
    "        key_counter=1\n",
    "        for key, count in current_dist_dict.items():\n",
    "            print(key, count)\n",
    "            key_minus_one_word, key_minus_one_count=NextWordProbability(sample, key)\n",
    "            if key_counter==1:\n",
    "                current_dist_word = key_minus_one_word\n",
    "                current_dist_count = key_minus_one_count\n",
    "                print(key_counter, current_dist_word, current_dist_count)\n",
    "            else:\n",
    "                current_dist_word=np.concatenate((current_dist_word, key_minus_one_word))\n",
    "                current_dist_count=np.concatenate((current_dist_count, key_minus_one_count))\n",
    "                print(key_counter, current_dist_word, current_dist_count)\n",
    "            key_counter+=1\n",
    "        \n",
    "        current_dist_unique, current_dist_unique_count=unique_sum(current_dist_word, current_dist_count)\n",
    "        current_dist_dict=dict(zip(current_dist_unique,current_dist_unique_count))\n",
    "        current_dist+=1\n",
    "        output=max(current_dist_dict, key=current_dist_dict.get)\n",
    "#     output_dict=zip(new_minus_one_word, new_minus_one_prob)\n",
    "#     return current_dist_unique,current_dist_prob\n",
    "    return current_dist_dict, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NextWordProbability2(sampletext,word):\n",
    "#     sampletext=re.sub('[\\W]+', ' ', sampletext)\n",
    "#     word_array=np.array(sampletext.lower().split())\n",
    "    word_array=np.array(sampletext.split())\n",
    "    word_pos=np.argwhere(word_array==word)\n",
    "    next_word=word_array[word_pos+1]\n",
    "    next_word_unique, next_word_count=np.unique(next_word, return_counts=True)\n",
    "    next_word_prob=np.float32(next_word_count)/next_word_count.sum()\n",
    "#     output_dict=dict(zip(next_word_unique,next_word_prob))\n",
    "    return next_word_unique, next_word_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LaterWords2(sample,word,distance):\n",
    "    '''@param sample: a sample of text to draw from\n",
    "    @param word: a word occuring before a corrupted sequence\n",
    "    @param distance: how many words later to estimate (i.e. 1 for the next word, 2 for the word after that)\n",
    "    @returns: a single word which is the most likely possibility\n",
    "    '''\n",
    "#     print(word, distance)\n",
    "    output_dict={}\n",
    "    minus_one_word, minus_one_prob=NextWordProbability2(sample, word)\n",
    "    minus_one_dict=dict(zip(minus_one_word,minus_one_prob))\n",
    "    # TODO: Given a word, collect the relative probabilities of possible following words\n",
    "    # from @sample. You may want to import your code from the maximum likelihood exercise. \n",
    "    if distance==1: return minus_one_dict\n",
    "\n",
    "    # TODO: Repeat the above process--for each distance beyond 1, evaluate the words that\n",
    "    # might come after each word, and combine them weighting by relative probability\n",
    "    # into an estimate of what might appear next.\n",
    "    current_dist=2\n",
    "    current_dist_dict=minus_one_dict\n",
    "#     print(current_dist_dict)\n",
    "    while current_dist<=distance:\n",
    "        # print('calculating probablity for distance {: d}'.format(current_dist))\n",
    "        key_counter=1\n",
    "        for key, prob in current_dist_dict.items():\n",
    "            print(key, prob)\n",
    "            key_minus_one_word, key_minus_one_prob=NextWordProbability(sample, key)\n",
    "            if key_counter==1:\n",
    "                current_dist_word = key_minus_one_word\n",
    "                current_dist_prob = key_minus_one_count*prob\n",
    "                print(key_counter, current_dist_word, current_dist_prob)\n",
    "            else:\n",
    "                current_dist_word=np.concatenate((current_dist_word, key_minus_one_word))\n",
    "                current_dist_prob=np.concatenate((current_dist_prob, key_minus_one_prob*prob))\n",
    "                print(key_counter, current_dist_word, current_dist_prob)\n",
    "            key_counter+=1\n",
    "        \n",
    "        current_dist_unique, current_dist_unique_prob=unique_sum(current_dist_word, current_dist_prob)\n",
    "        current_dist_dict=dict(zip(current_dist_unique,current_dist_unique_prob))\n",
    "        current_dist+=1\n",
    "        output=max(current_dist_dict, key=current_dist_dict.get)\n",
    "#     output_dict=zip(new_minus_one_word, new_minus_one_prob)\n",
    "#     return current_dist_unique,current_dist_prob\n",
    "    return current_dist_dict, output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('we', 0.07692308)\n",
      "(1, array(['can', 'need', 'sorta'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308]))\n",
      "('I', 0.15384616)\n",
      "(2, array(['can', 'need', 'sorta', 'almost'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232]))\n",
      "('move', 0.15384616)\n",
      "(3, array(['can', 'need', 'sorta', 'almost', 'it', 'you'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232,\n",
      "        0.15384616,  0.15384616]))\n",
      "('ah,', 0.07692308)\n",
      "(4, array(['can', 'need', 'sorta', 'almost', 'it', 'you', 'we'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232,\n",
      "        0.15384616,  0.15384616,  0.07692308]))\n",
      "('jeans.', 0.07692308)\n",
      "(5, array(['can', 'need', 'sorta', 'almost', 'it', 'you', 'we', 'Oh,'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232,\n",
      "        0.15384616,  0.15384616,  0.07692308,  0.07692308]))\n",
      "('remember:', 0.07692308)\n",
      "(6, array(['can', 'need', 'sorta', 'almost', 'it', 'you', 'we', 'Oh,', 'next'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232,\n",
      "        0.15384616,  0.15384616,  0.07692308,  0.07692308,  0.07692308]))\n",
      "('wear', 0.07692308)\n",
      "(7, array(['can', 'need', 'sorta', 'almost', 'it', 'you', 'we', 'Oh,', 'next',\n",
      "       'a'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232,\n",
      "        0.15384616,  0.15384616,  0.07692308,  0.07692308,  0.07692308,\n",
      "        0.07692308]))\n",
      "('come', 0.23076923)\n",
      "(8, array(['can', 'need', 'sorta', 'almost', 'it', 'you', 'we', 'Oh,', 'next',\n",
      "       'a', 'in'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232,\n",
      "        0.15384616,  0.15384616,  0.07692308,  0.07692308,  0.07692308,\n",
      "        0.07692308,  0.6923077 ]))\n",
      "('pack', 0.07692308)\n",
      "(9, array(['can', 'need', 'sorta', 'almost', 'it', 'you', 'we', 'Oh,', 'next',\n",
      "       'a', 'in', 'up'], \n",
      "      dtype='|S10'), array([ 0.15384616,  0.07692308,  0.23076924,  0.07692308,  0.15384616,\n",
      "        0.07692308,  0.07692308,  0.07692308,  0.07692308,  0.30769232,\n",
      "        0.15384616,  0.15384616,  0.07692308,  0.07692308,  0.07692308,\n",
      "        0.07692308,  0.6923077 ,  0.07692308]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Oh,': 0.076923079788684845,\n",
       "  'a': 0.30769231915473938,\n",
       "  'almost': 0.076923079788684845,\n",
       "  'can': 0.15384615957736969,\n",
       "  'in': 0.15384615957736969,\n",
       "  'it': 0.15384615957736969,\n",
       "  'need': 0.076923079788684845,\n",
       "  'next': 0.076923079788684845,\n",
       "  'sorta': 0.23076923936605453,\n",
       "  'up': 0.15384615957736969,\n",
       "  'we': 0.076923079788684845,\n",
       "  'you': 0.076923079788684845},\n",
       " 'a')"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='and'\n",
    "LaterWords2(sample_memo, word,distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('and', 1.0)\n",
      "(1, array(['I', 'ah,', 'come', 'jeans.', 'move', 'pack', 'remember:', 'we',\n",
      "       'wear'], \n",
      "      dtype='|S10'), array([ 2.,  1.,  3.,  1.,  2.,  1.,  1.,  1.,  1.]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'I': 2.0,\n",
       "  'ah,': 1.0,\n",
       "  'come': 3.0,\n",
       "  'jeans.': 1.0,\n",
       "  'move': 2.0,\n",
       "  'pack': 1.0,\n",
       "  'remember:': 1.0,\n",
       "  'we': 1.0,\n",
       "  'wear': 1.0},\n",
       " 'come')"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='ahead'\n",
    "# current_dist_word, current_dist_count,output=LaterWords(sample_memo, word,distance=2)\n",
    "# current_dist_word, current_dist_count,output\n",
    "\n",
    "LaterWords2(sample_memo, word,distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ahh': 1, 'can': 1, 'have': 1, 'need': 1, 're': 1, 'sorta': 1}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='we'\n",
    "LaterWords(sample_memo, word,distance=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 2)\n",
      "('those', 2)\n",
      "(2, array(['job', 'time', 'item', 'time'], \n",
      "      dtype='|S5'), array([1, 1, 1, 1]))\n",
      "('that', 1)\n",
      "(3, array(['job', 'time', 'item', 'time', 'time'], \n",
      "      dtype='|S5'), array([1, 1, 1, 1, 1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'item': 1, 'job': 1, 'time': 3}, 'time')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_memo2 = '''for this time, for that time, for this job, for those item, for those time'''\n",
    "\n",
    "word='for'\n",
    "\n",
    "# current_dist_word, current_dist_count,output=LaterWords(sample_memo, word,distance=2)\n",
    "# current_dist_word, current_dist_count,output\n",
    "\n",
    "LaterWords(sample_memo2, word,distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('and', 6)\n",
      "(1, array(['I', 'ah,', 'come', 'jeans.', 'move', 'pack', 'remember:', 'we',\n",
      "       'wear'], \n",
      "      dtype='|S10'), array([2, 1, 3, 1, 2, 1, 1, 1, 1]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'I': 2,\n",
       "  'ah,': 1,\n",
       "  'come': 3,\n",
       "  'jeans.': 1,\n",
       "  'move': 2,\n",
       "  'pack': 1,\n",
       "  'remember:': 1,\n",
       "  'we': 1,\n",
       "  'wear': 1},\n",
       " 'come')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='ahead'\n",
    "# current_dist_word, current_dist_count,output=LaterWords(sample_memo, word,distance=2)\n",
    "# current_dist_word, current_dist_count,output\n",
    "\n",
    "LaterWords(sample_memo, word,distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word='and'\n",
    "LaterWords(sample_memo, word,distance=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ah', 'come', 'i', 'jeans', 'move', 'pack', 'remember', 'we', 'wear'], \n",
       "       dtype='|S10'), array([1, 3, 2, 1, 2, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=np.array(['ah', 'come', 'i', 'jeans', 'come', 'jean'])\n",
    "b=np.array([1,3,2,1,3,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[np.argwhere(a=='come')].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ah', 'come', 'i', 'jean', 'jeans'], \n",
       "      dtype='|S5')"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unique_list=[]\n",
    "for i in np.unique(a):\n",
    "    unique_list.append([i, b[np.argwhere(a==i)].sum()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1', '6'], \n",
       "      dtype='|S5')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(unique_list)[0:2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shape mismatch: objects cannot be broadcast to a single shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-114-50952a488025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: shape mismatch: objects cannot be broadcast to a single shape"
     ]
    }
   ],
   "source": [
    "b[np.argwhere(a==np.unique(a))].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'come'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "argmax=np.argmax(current_dist_count)\n",
    "current_dist_word[argmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['and'], \n",
       "       dtype='|S10'), array([6]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NextWordProbability(sample_memo, 'ahead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['ah', 'come', 'i', 'jeans', 'move', 'pack', 'remember', 'we', 'wear'], \n",
       "       dtype='|S10'), array([1, 3, 2, 1, 2, 1, 1, 1, 1]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NextWordProbability(sample_memo, 'and')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(output_dict, key=output_dict.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 0.25, 'this': 0.5, 'those': 0.25}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='for'\n",
    "test_dict=LaterWords(sample_memo, word,distance=1)\n",
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this 0.5\n",
      "those 0.25\n",
      "that 0.25\n"
     ]
    }
   ],
   "source": [
    "for key, v in test_dict.items():\n",
    "    print(key,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 0.5), ('those', 0.25), ('that', 0.25)]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "remember\n",
      "i\n",
      "ah\n",
      "move\n",
      "jeans\n",
      "wear\n",
      "come\n",
      "pack\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "[print(y) for x,y in zip(test_dict.keys(), test_dict.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=np.array(['a','b','c'])\n",
    "b=np.array(['e','f','g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a', 'b', 'c', 'e', 'f', 'g'], \n",
       "      dtype='|S1')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def NextWordProbability(sampletext,word):\n",
    "    sampletext=re.sub('[\\W]+', ' ', sampletext)\n",
    "    word_array=np.array(sampletext.lower().split())\n",
    "    word_pos=np.argwhere(word_array==word)\n",
    "    next_word=word_array[word_pos+1]\n",
    "    next_word_unique, next_word_count=np.unique(next_word, return_counts=True)\n",
    "    output_dict=dict(zip(next_word_unique,next_word_count))\n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 0.2, 'this': 0.40000001, 'those': 0.40000001}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word='for'\n",
    "# a,b=NextWordProbability(sample_memo, word)\n",
    "NextWordProbability(sample_memo, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-7b6e3841ba9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNextWordProbability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_memo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-145-fc57156c61f4>\u001b[0m in \u001b[0;36mNextWordProbability\u001b[0;34m(sampletext, word)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mnext_word_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_word_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mnext_word_prob\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnext_word_count\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moutput_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_unique\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_word_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m#     return next_word_unique, next_word_prob\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-33edcd87d605>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "dict(zip(a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for this time, for that time, for this job, for those item,\\nfor those time'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_memo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['could', 'downstairs', 'know', 'to', 'want'], \n",
       "       dtype='|S10'), array([2, 1, 1, 3, 1]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word_unique, next_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25 ,  0.125,  0.125,  0.375,  0.125], dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(next_word_count)/next_word_count.sum()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:snakes2]",
   "language": "python",
   "name": "conda-env-snakes2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
