{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "# \n",
    "# As with the previous perceptron exercises, you will complete some of the core\n",
    "# methods of a sigmoid unit class.\n",
    "#\n",
    "# There are two functions for you to finish:\n",
    "# First, in activate(), write the sigmoid activation function.\n",
    "# Second, in update(), write the gradient descent update rule. Updates should be\n",
    "#   performed online, revising the weights after each data point.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with sigmoid activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1])):\n",
    "        \"\"\"\n",
    "        Initialize weights based on input arguments. Note that no type-checking\n",
    "        is being performed here for simplicity of code.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "\n",
    "        # NOTE: You do not need to worry about these two attribues for this\n",
    "        # programming quiz, but these will be useful for if you want to create\n",
    "        # a network out of these sigmoid units!\n",
    "        self.last_input = 0 # strength of last input\n",
    "        self.delta      = 0 # error signal\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a sigmoid unit with given inputs based on unit\n",
    "        weights.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        # First calculate the strength of the input signal.\n",
    "        strength = np.dot(values, self.weights)\n",
    "        self.last_input = strength\n",
    "        \n",
    "        # TODO: Modify strength using the sigmoid activation function and\n",
    "        # return as output signal.\n",
    "        # HINT: You may want to create a helper function to compute the\n",
    "        #   logistic function since you will need it for the update function.\n",
    "        \n",
    "        return 1 / (1 + np.exp(-strength))\n",
    "    \n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to gradient descent using\n",
    "        these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: for each data point...\n",
    "        for X, y_true in zip(values, train):\n",
    "            # obtain the output signal for that point\n",
    "            y_pred = self.activate(X)\n",
    "            error=y_true-y_pred\n",
    "            print(X, y_true, y_pred, error)\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "            # TODO: compute derivative of logistic function at input strength\n",
    "            # Recall: d/dx logistic(x) = logistic(x)*(1-logistic(x))\n",
    "            \n",
    "            self.weights+=eta*error*X\n",
    "            \n",
    "            # TODO: update self.weights based on learning rate, signal accuracy,\n",
    "            # function slope (derivative) and input value\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1, 2, 3]), 0, 0.88079707797788231, -0.88079707797788231)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "A few tests to make sure that the perceptron class performs as expected.\n",
    "Nothing should show up in the output if all the assertions pass.\n",
    "\"\"\"\n",
    "def sum_almost_equal(array1, array2, tol = 1e-5):\n",
    "    return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "u1 = Sigmoid(weights=[3,-2,1])\n",
    "assert abs(u1.activate(np.array([1,2,3])) - 0.880797) < 1e-5\n",
    "\n",
    "u1.update(np.array([[1,2,3]]),np.array([0]))\n",
    "# assert sum_almost_equal(u1.weights, np.array([2.990752, -2.018496, 0.972257]))\n",
    "\n",
    "#     u2 = Sigmoid(weights=[0,3,-1])\n",
    "#     u2.update(np.array([[-3,-1,2],[2,1,2]]),np.array([1,0]))\n",
    "#     assert sum_almost_equal(u2.weights, np.array([-0.030739, 2.984961, -1.027437]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, -2, 1]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:snakes2]",
   "language": "python",
   "name": "conda-env-snakes2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
